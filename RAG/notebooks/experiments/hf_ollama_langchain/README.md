# Hugging Face â†’ Ollama â†’ LangChain í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  Ollamaë¡œ ë³€í™˜í•œ í›„ LangChainìœ¼ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### 1. Docker Compose í™˜ê²½ ì„¤ì •
```bash
# RAG í™˜ê²½ ì‹œì‘
cd AI/RAG
docker-compose up -d ollama
```

### 2. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# Jupyter ì»¨í…Œì´ë„ˆì— ì ‘ì†
docker exec -it jupyter bash

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements-hf-ollama.txt
```

### 3. Hugging Face í† í° ì„¤ì • (ì„ íƒì‚¬í•­)
```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export HUGGING_FACE_HUB_TOKEN="your_token_here"
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ë°©ë²• 1: Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
# Jupyter ì»¨í…Œì´ë„ˆì—ì„œ ì‹¤í–‰
python hf_to_ollama_test.py
```

### ë°©ë²• 2: Jupyter ë…¸íŠ¸ë¶ ì‹¤í–‰
```bash
# Jupyter ë…¸íŠ¸ë¶ ì‹œì‘
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8888 ì ‘ì†
# hf_ollama_langchain_demo.ipynb íŒŒì¼ ì‹¤í–‰
```

## ğŸ“ ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤

### 1. ëª¨ë¸ íŒŒì¼
- `./ollama/models/`: ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤ì´ ì €ì¥ë˜ëŠ” í´ë”
- `./ollama/models.json`: ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡

### 2. ê²°ê³¼ íŒŒì¼
- `ollama_test_results.json`: í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì €ì¥ë˜ëŠ” JSON íŒŒì¼

## ğŸ”§ ì£¼ìš” ê¸°ëŠ¥

### 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```python
# Hugging Face ëª¨ë¸ì„ Ollamaë¡œ ë‹¤ìš´ë¡œë“œ
converter = HFToOllamaConverter()
converter.pull_model_from_hf("llama2", "meta-llama/Llama-2-7b-chat-hf")
```

### 2. ì¶”ë¡  í…ŒìŠ¤íŠ¸
```python
# LangChainìœ¼ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
llm = Ollama(model="llama2", temperature=0.7)
response = llm("ì•ˆë…•í•˜ì„¸ìš”!")
print(response)
```

### 3. ì±„íŒ… í…ŒìŠ¤íŠ¸
```python
# ì±„íŒ… ëª¨ë¸ í…ŒìŠ¤íŠ¸
chat_model = ChatOllama(model="llama2", temperature=0.7)
messages = [
    SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    HumanMessage(content="íŒŒì´ì¬ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”.")
]
response = chat_model(messages)
print(response.content)
```

## ğŸ“Š ì§€ì›í•˜ëŠ” ëª¨ë¸ë“¤

### ê¸°ë³¸ ëª¨ë¸
- **Llama2**: `meta-llama/Llama-2-7b-chat-hf`
- **CodeLlama**: `codellama/CodeLlama-7b-Instruct-hf`
- **Mistral**: `mistralai/Mistral-7B-Instruct-v0.2`

### í•œêµ­ì–´ ëª¨ë¸
- **KoAlpaca**: `beomi/KoAlpaca-Polyglot-12.8B`
- **KoBART**: `gogamza/kobart-base-v2`

## ğŸ› ï¸ ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±

### Modelfile ìƒì„±
```python
# ì»¤ìŠ¤í…€ Modelfile ìƒì„±
modelfile_path = converter.create_custom_modelfile(
    model_name="my-model",
    hf_model_id="your-model-id",
    template="ë‹¹ì‹ ì€ {{ .System }}\n\nì‚¬ìš©ì: {{ .Prompt }}\n\nì–´ì‹œìŠ¤í„´íŠ¸: "
)
```

### ëª¨ë¸ ìƒì„±
```python
# Modelfileë¡œë¶€í„° ëª¨ë¸ ìƒì„±
converter.create_model_from_modelfile("my-model", modelfile_path)
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

### 1. ëª¨ë¸ ëª©ë¡ í™•ì¸
```bash
# APIë¡œ í™•ì¸
curl http://localhost:11434/api/tags

# Pythonìœ¼ë¡œ í™•ì¸
converter.list_installed_models()
```

### 2. ëª¨ë¸ ì œê±°
```bash
# APIë¡œ ì œê±°
curl -X DELETE http://localhost:11434/api/delete \
  -H "Content-Type: application/json" \
  -d '{"name": "model-name"}'
```

### 3. ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
```bash
# ëª¨ë¸ í´ë” í¬ê¸° í™•ì¸
du -sh ./ollama/
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
- **7B ëª¨ë¸**: ìµœì†Œ 8GB RAM
- **13B ëª¨ë¸**: ìµœì†Œ 16GB RAM
- **70B ëª¨ë¸**: ìµœì†Œ 64GB RAM

### 2. ë””ìŠ¤í¬ ê³µê°„
- **7B ëª¨ë¸**: ~4GB
- **13B ëª¨ë¸**: ~8GB
- **70B ëª¨ë¸**: ~40GB

### 3. ë‹¤ìš´ë¡œë“œ ì‹œê°„
- ì¸í„°ë„· ì†ë„ì— ë”°ë¼ 10ë¶„~1ì‹œê°„ ì†Œìš”
- ëª¨ë¸ í¬ê¸°ê°€ í´ìˆ˜ë¡ ë” ì˜¤ë˜ ê±¸ë¦¼

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
docker ps | grep ollama

# ì„œë²„ ì¬ì‹œì‘
docker-compose restart ollama
```

### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# ë¡œê·¸ í™•ì¸
docker logs ollama

# í† í° ì„¤ì • í™•ì¸
echo $HUGGING_FACE_HUB_TOKEN
```

### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ í™•ì¸
docker stats ollama

# ë‹¤ë¥¸ ëª¨ë¸ ì œê±°
curl -X DELETE http://localhost:11434/api/delete \
  -H "Content-Type: application/json" \
  -d '{"name": "large-model"}'
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. GPU ì‚¬ìš© (ì„ íƒì‚¬í•­)
```yaml
# docker-compose.ymlì— GPU ì„¤ì • ì¶”ê°€
services:
  ollama:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### 2. ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì •
```python
# ì¶”ë¡  ì†ë„ vs í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„
llm = Ollama(
    model="llama2",
    temperature=0.1,  # ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ
    top_p=0.9,
    top_k=40
)
```

## ğŸ“š ì¶”ê°€ ìë£Œ

- [Ollama ê³µì‹ ë¬¸ì„œ](https://ollama.ai/docs)
- [LangChain Ollama í†µí•©](https://python.langchain.com/docs/integrations/llms/ollama)
- [Hugging Face ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/models)

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê¸°ëŠ¥ ìš”ì²­ì€ ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
Pull Requestë„ í™˜ì˜í•©ë‹ˆë‹¤! 