#!/usr/bin/env python3
"""
Hugging Face ëª¨ë¸ì„ Ollamaë¡œ ë³€í™˜í•˜ê³  LangChainìœ¼ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
"""

import os
import json
import requests
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any
import time

# LangChain ê´€ë ¨ import
from langchain.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.schema import HumanMessage, SystemMessage
from langchain.chat_models import ChatOllama

class HFToOllamaConverter:
    def __init__(self, ollama_host: str = "http://localhost:11434"):
        self.ollama_host = ollama_host
        self.models_dir = Path("./ollama/models")
        self.models_dir.mkdir(exist_ok=True)
        
    def check_ollama_status(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def list_installed_models(self) -> list:
        """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags")
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except Exception as e:
            print(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def pull_model_from_hf(self, model_name: str, hf_model_id: str) -> bool:
        """
        Hugging Face ëª¨ë¸ì„ Ollamaë¡œ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            model_name: Ollamaì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            hf_model_id: Hugging Face ëª¨ë¸ ID (ì˜ˆ: "microsoft/DialoGPT-medium")
        """
        try:
            print(f"ğŸ¤– {hf_model_id} ëª¨ë¸ì„ Ollamaë¡œ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            
            # Ollama pull ëª…ë ¹ì–´ ì‹¤í–‰
            cmd = [
                "ollama", "pull", 
                f"{model_name}:latest",
                "--insecure-registry"
            ]
            
            # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Hugging Face í† í°ì´ ìˆëŠ” ê²½ìš°)
            env = os.environ.copy()
            if "HUGGING_FACE_HUB_TOKEN" in env:
                env["HF_TOKEN"] = env["HUGGING_FACE_HUB_TOKEN"]
            
            # ëª…ë ¹ì–´ ì‹¤í–‰
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                env=env,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                print(f"âœ… {model_name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                return True
            else:
                print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print("âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def create_custom_modelfile(self, model_name: str, hf_model_id: str, template: str = None) -> str:
        """
        ì»¤ìŠ¤í…€ Modelfile ìƒì„±
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            hf_model_id: Hugging Face ëª¨ë¸ ID
            template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        """
        modelfile_content = f"""FROM {hf_model_id}

# ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM \"\"\"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\"\"\"

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
TEMPLATE \"\"\"{template or "{{ .System }}\n\nì‚¬ìš©ì: {{ .Prompt }}\n\nì–´ì‹œìŠ¤í„´íŠ¸: "}\"\"\"

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
"""
        
        modelfile_path = f"./modelfile_{model_name}"
        with open(modelfile_path, 'w', encoding='utf-8') as f:
            f.write(modelfile_content)
        
        return modelfile_path
    
    def create_model_from_modelfile(self, model_name: str, modelfile_path: str) -> bool:
        """Modelfileë¡œë¶€í„° ëª¨ë¸ ìƒì„±"""
        try:
            print(f"ğŸ”¨ {model_name} ëª¨ë¸ ìƒì„± ì¤‘...")
            
            cmd = ["ollama", "create", model_name, "-f", modelfile_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… {model_name} ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
                return True
            else:
                print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def test_model_inference(self, model_name: str, test_prompts: list) -> Dict[str, Any]:
        """ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
        results = {}
        
        try:
            # Ollama LLM ì´ˆê¸°í™”
            llm = Ollama(
                model=model_name,
                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
                temperature=0.7
            )
            
            print(f"\nğŸ§ª {model_name} ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")
            
            for i, prompt in enumerate(test_prompts, 1):
                print(f"\n--- í…ŒìŠ¤íŠ¸ {i} ---")
                print(f"í”„ë¡¬í”„íŠ¸: {prompt}")
                print("ì‘ë‹µ:")
                
                start_time = time.time()
                try:
                    response = llm(prompt)
                    end_time = time.time()
                    
                    results[f"test_{i}"] = {
                        "prompt": prompt,
                        "response": response,
                        "time_taken": end_time - start_time,
                        "status": "success"
                    }
                    
                except Exception as e:
                    results[f"test_{i}"] = {
                        "prompt": prompt,
                        "error": str(e),
                        "status": "error"
                    }
                    print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                
                print("-" * 50)
            
            return results
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def test_chat_model(self, model_name: str, test_conversations: list) -> Dict[str, Any]:
        """ì±„íŒ… ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        results = {}
        
        try:
            # ChatOllama ì´ˆê¸°í™”
            chat_model = ChatOllama(
                model=model_name,
                temperature=0.7
            )
            
            print(f"\nğŸ’¬ {model_name} ì±„íŒ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")
            
            for i, conversation in enumerate(test_conversations, 1):
                print(f"\n--- ì±„íŒ… í…ŒìŠ¤íŠ¸ {i} ---")
                
                messages = []
                for msg in conversation:
                    if msg["role"] == "system":
                        messages.append(SystemMessage(content=msg["content"]))
                    elif msg["role"] == "user":
                        messages.append(HumanMessage(content=msg["content"]))
                
                print(f"ëŒ€í™”: {[msg.content for msg in messages]}")
                print("ì‘ë‹µ:")
                
                start_time = time.time()
                try:
                    response = chat_model(messages)
                    end_time = time.time()
                    
                    results[f"chat_test_{i}"] = {
                        "conversation": conversation,
                        "response": response.content,
                        "time_taken": end_time - start_time,
                        "status": "success"
                    }
                    
                    print(f"AI: {response.content}")
                    
                except Exception as e:
                    results[f"chat_test_{i}"] = {
                        "conversation": conversation,
                        "error": str(e),
                        "status": "error"
                    }
                    print(f"âŒ ì±„íŒ… ì‹¤íŒ¨: {e}")
                
                print("-" * 50)
            
            return results
            
        except Exception as e:
            print(f"âŒ ì±„íŒ… ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Hugging Face â†’ Ollama â†’ LangChain í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # ì»¨ë²„í„° ì´ˆê¸°í™”
    converter = HFToOllamaConverter()
    
    # Ollama ì„œë²„ ìƒíƒœ í™•ì¸
    if not converter.check_ollama_status():
        print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("docker-compose up -d ollama ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
        return
    
    print("âœ… Ollama ì„œë²„ ì—°ê²° í™•ì¸ë¨")
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ í™•ì¸
    installed_models = converter.list_installed_models()
    print(f"ğŸ“‹ ì„¤ì¹˜ëœ ëª¨ë¸: {installed_models}")
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„¤ì •
    test_models = [
        {
            "name": "llama2",
            "hf_id": "meta-llama/Llama-2-7b-chat-hf",
            "description": "Llama2 7B ì±„íŒ… ëª¨ë¸"
        },
        {
            "name": "codellama", 
            "hf_id": "codellama/CodeLlama-7b-Instruct-hf",
            "description": "CodeLlama 7B ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸"
        }
    ]
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?",
        "íŒŒì´ì¬ìœ¼ë¡œ Hello Worldë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?",
        "í•œêµ­ì˜ ì „í†µ ë¬¸í™”ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]
    
    # í…ŒìŠ¤íŠ¸ ëŒ€í™”
    test_conversations = [
        [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": "íŒŒì´ì¬ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”. ì–´ë–»ê²Œ ì‹œì‘í•˜ë©´ ì¢‹ì„ê¹Œìš”?"}
        ],
        [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì½”ë”© íŠœí„°ì…ë‹ˆë‹¤."},
            {"role": "user", "content": "ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
        ]
    ]
    
    all_results = {}
    
    for model_config in test_models:
        model_name = model_config["name"]
        hf_id = model_config["hf_id"]
        
        print(f"\n{'='*60}")
        print(f"ğŸ”§ {model_config['description']} ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        # ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if model_name in installed_models:
            print(f"âœ… {model_name} ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            success = converter.pull_model_from_hf(model_name, hf_id)
            if not success:
                print(f"âŒ {model_name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰...")
                continue
        
        # ì¶”ë¡  í…ŒìŠ¤íŠ¸
        print(f"\nğŸ§ª {model_name} ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
        inference_results = converter.test_model_inference(model_name, test_prompts)
        all_results[f"{model_name}_inference"] = inference_results
        
        # ì±„íŒ… í…ŒìŠ¤íŠ¸
        print(f"\nğŸ’¬ {model_name} ì±„íŒ… í…ŒìŠ¤íŠ¸...")
        chat_results = converter.test_chat_model(model_name, test_conversations)
        all_results[f"{model_name}_chat"] = chat_results
        
        print(f"\nâœ… {model_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # ê²°ê³¼ ì €ì¥
    with open("ollama_test_results.json", "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ê°€ ollama_test_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    for model_name in [m["name"] for m in test_models]:
        if f"{model_name}_inference" in all_results:
            inference_success = sum(1 for r in all_results[f"{model_name}_inference"].values() 
                                  if r.get("status") == "success")
            print(f"  {model_name} ì¶”ë¡ : {inference_success}/{len(test_prompts)} ì„±ê³µ")
        
        if f"{model_name}_chat" in all_results:
            chat_success = sum(1 for r in all_results[f"{model_name}_chat"].values() 
                             if r.get("status") == "success")
            print(f"  {model_name} ì±„íŒ…: {chat_success}/{len(test_conversations)} ì„±ê³µ")

if __name__ == "__main__":
    main() 