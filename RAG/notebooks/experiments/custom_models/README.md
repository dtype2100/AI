# Custom Models μ‹¤ν—

μ΄ λ””λ ‰ν† λ¦¬λ” μ»¤μ¤ν…€ λ¨λΈ κ°λ°κ³Ό μ‹¤ν—μ„ μ„ν• κ³µκ°„μ…λ‹λ‹¤.

## π― λ©μ 

- λ¨λΈ νμΈνλ‹ μ‹¤ν—
- ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§
- μ»¤μ¤ν…€ λ¨λΈ κ°λ° λ° ν‰κ°€

## π“ μμƒ νμΌ κµ¬μ΅°

```
custom_models/
β”β”€β”€ fine_tuning/                # λ¨λΈ νμΈνλ‹
β”‚   β”β”€β”€ data_preparation.py     # λ°μ΄ν„° μ „μ²λ¦¬
β”‚   β”β”€β”€ training_script.py      # ν•™μµ μ¤ν¬λ¦½νΈ
β”‚   β”β”€β”€ evaluation.py           # νμΈνλ‹ λ¨λΈ ν‰κ°€
β”‚   β””β”€β”€ README.md
β”β”€β”€ prompt_engineering/         # ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§
β”‚   β”β”€β”€ prompt_templates.py     # ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ
β”‚   β”β”€β”€ prompt_testing.py       # ν”„λ΅¬ν”„νΈ ν…μ¤νΈ
β”‚   β”β”€β”€ prompt_optimization.py  # ν”„λ΅¬ν”„νΈ μµμ ν™”
β”‚   β””β”€β”€ README.md
β”β”€β”€ model_adaptation/           # λ¨λΈ μ μ‘
β”‚   β”β”€β”€ domain_adaptation.py    # λ„λ©”μΈ μ μ‘
β”‚   β”β”€β”€ task_adaptation.py      # νƒμ¤ν¬ μ μ‘
β”‚   β””β”€β”€ README.md
β”β”€β”€ config/                     # μ„¤μ • νμΌ
β”β”€β”€ data/                       # λ°μ΄ν„°μ…‹
β”β”€β”€ models/                     # ν›λ ¨λ λ¨λΈ
β”β”€β”€ results/                    # μ‹¤ν— κ²°κ³Ό
β””β”€β”€ logs/                       # λ΅κ·Έ νμΌ
```

## π”§ μ£Όμ” κΈ°λ¥

### 1. Fine-tuning
- **λ°μ΄ν„° μ¤€λΉ„**: λ„λ©”μΈλ³„ λ°μ΄ν„° μμ§‘ λ° μ „μ²λ¦¬
- **λ¨λΈ ν›λ ¨**: LoRA, QLoRA λ“± ν¨μ¨μ  νμΈνλ‹
- **λ¨λΈ ν‰κ°€**: μ„±λ¥ μΈ΅μ • λ° λΉ„κµ

### 2. Prompt Engineering
- **ν…ν”λ¦Ώ μ„¤κ³„**: λ‹¤μ–‘ν• ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ
- **A/B ν…μ¤νΈ**: ν”„λ΅¬ν”„νΈ ν¨κ³Ό λΉ„κµ
- **μµμ ν™”**: μλ™ ν”„λ΅¬ν”„νΈ μµμ ν™”

### 3. Model Adaptation
- **λ„λ©”μΈ μ μ‘**: νΉμ • λ„λ©”μΈμ— λ§μ¶ λ¨λΈ μ΅°μ •
- **νƒμ¤ν¬ μ μ‘**: νΉμ • νƒμ¤ν¬μ— μµμ ν™”
- **μ„±λ¥ νλ‹**: ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”

## π“‹ μ‚¬μ© λ°©λ²•

### Fine-tuning
```bash
cd fine_tuning

# λ°μ΄ν„° μ¤€λΉ„
python data_preparation.py

# λ¨λΈ ν›λ ¨
python training_script.py

# λ¨λΈ ν‰κ°€
python evaluation.py
```

### Prompt Engineering
```bash
cd prompt_engineering

# ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ μƒμ„±
python prompt_templates.py

# ν”„λ΅¬ν”„νΈ ν…μ¤νΈ
python prompt_testing.py

# ν”„λ΅¬ν”„νΈ μµμ ν™”
python prompt_optimization.py
```

## π“ ν‰κ°€ μ§€ν‘

### Fine-tuning ν‰κ°€
- **μ •ν™•λ„**: νƒμ¤ν¬λ³„ μ •ν™•λ„
- **μ†μ‹¤**: ν›λ ¨/κ²€μ¦ μ†μ‹¤
- **μΌλ°ν™”**: μƒλ΅μ΄ λ°μ΄ν„°μ— λ€ν• μ„±λ¥

### Prompt Engineering ν‰κ°€
- **μ‘λ‹µ ν’μ§**: μ£Όκ΄€μ  ν’μ§ ν‰κ°€
- **μΌκ΄€μ„±**: λ™μΌ μ…λ ¥μ— λ€ν• μ‘λ‹µ μΌκ΄€μ„±
- **ν¨μ¨μ„±**: ν† ν° μ‚¬μ©λ‰ λ€λΉ„ ν’μ§

## π› οΈ κΈ°μ  μ¤νƒ

### Fine-tuning
- **PEFT**: Parameter-Efficient Fine-Tuning
- **LoRA**: Low-Rank Adaptation
- **QLoRA**: Quantized LoRA
- **Transformers**: Hugging Face Transformers

### Prompt Engineering
- **LangChain**: ν”„λ΅¬ν”„νΈ μ²΄μΈ κ΄€λ¦¬
- **OpenAI**: GPT λ¨λΈ ν™μ©
- **Ollama**: λ΅μ»¬ λ¨λΈ ν™μ©

## π“ μ‹¤ν— μμ‹

### 1. ν•κµ­μ–΄ μ±—λ΄‡ νμΈνλ‹
```python
# ν•κµ­μ–΄ λ€ν™” λ°μ΄ν„°λ΅ Llama2 νμΈνλ‹
python fine_tuning/training_script.py \
  --model_name "llama2" \
  --dataset "korean_chat" \
  --method "lora"
```

### 2. ν”„λ΅¬ν”„νΈ μµμ ν™”
```python
# μλ™ ν”„λ΅¬ν”„νΈ μµμ ν™”
python prompt_engineering/prompt_optimization.py \
  --task "summarization" \
  --model "gpt-3.5-turbo" \
  --iterations 100
```

## π” λ¨λ‹ν„°λ§

- **TensorBoard**: ν›λ ¨ κ³Όμ • μ‹κ°ν™”
- **Weights & Biases**: μ‹¤ν— μ¶”μ 
- **MLflow**: λ¨λΈ λ²„μ „ κ΄€λ¦¬ 